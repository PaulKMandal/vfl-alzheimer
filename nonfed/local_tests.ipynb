{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522dc4c8-bb69-47c8-92f3-2e15434666e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These dataloaders are taken from FATE\n",
    "from dataset.table import TableDataset\n",
    "from dataset.image import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5c2869-db87-4c59-bdd5-d1680fce962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_train_path = '../examples/data/oasis2train/oasis2.csv'\n",
    "host_train_path = '../examples/data/oasis2train/mri/'\n",
    "\n",
    "guest_train = TableDataset()\n",
    "guest_train.load(guest_train_path)\n",
    "host_train = ImageDataset(return_label=False)\n",
    "host_train.load(host_train_path)\n",
    "\n",
    "guest_val_path = '../examples/data/oasis2val/oasis2.csv'\n",
    "host_val_path = '../examples/data/oasis2val/mri/'\n",
    "\n",
    "guest_val = TableDataset()\n",
    "guest_val.load(guest_val_path)\n",
    "host_val = ImageDataset(return_label=False)\n",
    "host_val.load(host_val_path)\n",
    "\n",
    "guest_test_path = '../examples/data/oasis2test/oasis2.csv'\n",
    "host_test_path = '../examples/data/oasis2test/mri/'\n",
    "\n",
    "guest_test = TableDataset()\n",
    "guest_test.load(guest_test_path)\n",
    "host_test = ImageDataset(return_label=False)\n",
    "host_test.load(host_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a772423-cf72-4a30-8283-eda427f10b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mri  oasis2.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../examples/data/oasis2train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4999e874-07a6-46f7-8c4e-00c0d0045e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "class BottomHost(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BottomHost, self).__init__()\n",
    "        self.cuda = True\n",
    "        self.seq = t.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=3),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "            nn.AvgPool2d(kernel_size=3)\n",
    "        )\n",
    "\n",
    "        self.fc = t.nn.Sequential(   # extracted feature is a 8-dim embedding\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x\n",
    "        x = self.seq(x)\n",
    "        x = x.mean((2,3))\n",
    "        #x = x.flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed7d998-bd66-49c3-ab91-c2cb98b7bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "\n",
    "class BottomGuest(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BottomGuest, self).__init__()\n",
    "        self.fc = t.nn.Linear(12,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042a21d7-b228-49aa-a749-1d61dd66803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "\n",
    "class TopNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TopNet, self).__init__()\n",
    "        self.fc = t.nn.Sequential(   \n",
    "            nn.Linear(10, 3),\n",
    "            t.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677d33b2-0c6a-4b43-b679-d2bac782c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "\n",
    "class InteractiveLayer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(InteractiveLayer, self).__init__()\n",
    "        self.fc = t.nn.Linear(10,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51e3469-1519-45ee-b810-985f03b08e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "\n",
    "class CompositeModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CompositeModel, self).__init__()\n",
    "        self.cnn = BottomHost()\n",
    "        self.fnn = BottomGuest()\n",
    "        self.interactive = InteractiveLayer()\n",
    "        self.top = TopNet()\n",
    "\n",
    "    def forward(self, img, table):\n",
    "        x = self.cnn(img) + self.fnn(table)\n",
    "        x = self.interactive(x)\n",
    "        x = self.top(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, mode):\n",
    "        self.cnn.train(mode=mode)\n",
    "        self.fnn.train(mode=mode)\n",
    "        self.interactive.train(mode=mode)\n",
    "        self.top.train(mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70dca895-6d84-4867-b052-905fc7b9a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CompositeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47553194-5b99-4be5-959b-dd9e6f79528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_loss: tensor(0.6449, device='cuda:0')\n",
      "val_loss: tensor(0.6975, device='cuda:0')\n",
      "1\n",
      "train_loss: tensor(0.6487, device='cuda:0')\n",
      "val_loss: tensor(0.6997, device='cuda:0')\n",
      "2\n",
      "train_loss: tensor(0.6413, device='cuda:0')\n",
      "val_loss: tensor(0.6965, device='cuda:0')\n",
      "3\n",
      "train_loss: tensor(0.6453, device='cuda:0')\n",
      "val_loss: tensor(0.7014, device='cuda:0')\n",
      "4\n",
      "train_loss: tensor(0.6529, device='cuda:0')\n",
      "val_loss: tensor(0.7026, device='cuda:0')\n",
      "5\n",
      "train_loss: tensor(0.6651, device='cuda:0')\n",
      "val_loss: tensor(0.7200, device='cuda:0')\n",
      "6\n",
      "train_loss: tensor(0.6731, device='cuda:0')\n",
      "val_loss: tensor(0.7047, device='cuda:0')\n",
      "7\n",
      "train_loss: tensor(0.6392, device='cuda:0')\n",
      "val_loss: tensor(0.6985, device='cuda:0')\n",
      "8\n",
      "train_loss: tensor(0.6324, device='cuda:0')\n",
      "val_loss: tensor(0.6974, device='cuda:0')\n",
      "9\n",
      "train_loss: tensor(0.6312, device='cuda:0')\n",
      "val_loss: tensor(0.6983, device='cuda:0')\n",
      "10\n",
      "train_loss: tensor(0.6305, device='cuda:0')\n",
      "val_loss: tensor(0.6992, device='cuda:0')\n",
      "11\n",
      "train_loss: tensor(0.6297, device='cuda:0')\n",
      "val_loss: tensor(0.6998, device='cuda:0')\n",
      "12\n",
      "train_loss: tensor(0.6294, device='cuda:0')\n",
      "val_loss: tensor(0.6999, device='cuda:0')\n",
      "13\n",
      "train_loss: tensor(0.6293, device='cuda:0')\n",
      "val_loss: tensor(0.7000, device='cuda:0')\n",
      "14\n",
      "train_loss: tensor(0.6294, device='cuda:0')\n",
      "val_loss: tensor(0.7003, device='cuda:0')\n",
      "15\n",
      "train_loss: tensor(0.6292, device='cuda:0')\n",
      "val_loss: tensor(0.7005, device='cuda:0')\n",
      "16\n",
      "train_loss: tensor(0.6294, device='cuda:0')\n",
      "val_loss: tensor(0.7009, device='cuda:0')\n",
      "17\n",
      "train_loss: tensor(0.6298, device='cuda:0')\n",
      "val_loss: tensor(0.7027, device='cuda:0')\n",
      "18\n",
      "train_loss: tensor(0.6299, device='cuda:0')\n",
      "val_loss: tensor(0.7038, device='cuda:0')\n",
      "19\n",
      "train_loss: tensor(0.6284, device='cuda:0')\n",
      "val_loss: tensor(0.7013, device='cuda:0')\n",
      "20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m valid_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(epoch)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, (dem, lbl) \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     42\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m     dem \u001b[38;5;241m=\u001b[39m dem\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Research/vfl-alzheimer/nonfed/dataset/image.py:88\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtype(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     85\u001b[0m         item[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mtype(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_dtype)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_folder\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    247\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:933\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    887\u001b[0m ):\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "model = CompositeModel()\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "    \n",
    "loss = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = zip(DataLoader(host_train, batch_size=batch_size, shuffle=False), DataLoader(guest_train, batch_size=batch_size, shuffle=False))\n",
    "val_dataloader = zip(DataLoader(host_val, batch_size=batch_size, shuffle=False), DataLoader(guest_val, batch_size=batch_size, shuffle=False))\n",
    "test_dataloader = zip(DataLoader(host_test, batch_size=batch_size, shuffle=False), DataLoader(guest_test, batch_size=batch_size, shuffle=False))\n",
    "\n",
    "n_train = len(list(train_dataloader))\n",
    "n_val = len(list(val_dataloader))\n",
    "n_test = len(list(test_dataloader))\n",
    "\n",
    "\n",
    "#train_dataloader = train_dataloader.to(device)\n",
    "#val_dataloader = val_dataloader.to(device)\n",
    "#test_dataloader = test_dataloader.to(device)\n",
    "# Iterate through the DataLoader\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Wonky fix, the BLUF is don't zip dataloaders.\n",
    "    train_dataloader = zip(DataLoader(host_train, batch_size=batch_size, shuffle=False), DataLoader(guest_train, batch_size=batch_size, shuffle=False))\n",
    "    val_dataloader = zip(DataLoader(host_val, batch_size=batch_size, shuffle=False), DataLoader(guest_val, batch_size=batch_size, shuffle=False))\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    valid_accuracy = 0\n",
    "    print(epoch)\n",
    "    for img, (dem, lbl) in train_dataloader:\n",
    "        img = img.to(device)\n",
    "        dem = dem.to(device)\n",
    "        lbl = lbl.to(device).long().squeeze()\n",
    "        #lbl = t.nn.functional.one_hot(lbl.to(t.int64),3).to(device)\n",
    "        \n",
    "        model.train(mode=True)\n",
    "        output = model(img, dem)\n",
    "            \n",
    "        l = loss(output, lbl)\n",
    "            \n",
    "        train_loss += l.detach()/n_train\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('train_loss: ' + str(train_loss))\n",
    "\n",
    "    for img, (dem, lbl) in val_dataloader:\n",
    "        img = img.to(device)\n",
    "        dem = dem.to(device)\n",
    "        lbl = lbl.to(device).long().squeeze()\n",
    "        \n",
    "        model.train(mode=False)\n",
    "        output = model(img, dem)\n",
    "        l = loss(output, lbl)\n",
    "        val_loss += l.detach()/n_val\n",
    "    \n",
    "    print('val_loss: ' + str(val_loss))\n",
    "    \n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb91552c-33da-4d97-81e8-e9d5b5a465f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.9526),\n",
       " tensor(0.9262),\n",
       " tensor(0.9129),\n",
       " tensor(0.8174),\n",
       " tensor(0.7534),\n",
       " tensor(0.7473),\n",
       " tensor(0.7650),\n",
       " tensor(0.9416),\n",
       " tensor(0.7704),\n",
       " tensor(0.7066),\n",
       " tensor(0.7088),\n",
       " tensor(0.7044),\n",
       " tensor(0.7046),\n",
       " tensor(0.7032),\n",
       " tensor(0.7026),\n",
       " tensor(0.7020),\n",
       " tensor(0.7011),\n",
       " tensor(0.6994),\n",
       " tensor(0.6982),\n",
       " tensor(0.6977)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41d377-c405-40cd-9fd8-fb9e13c86576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
